{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "33c759e7-f891-41bc-8d80-e8c9a43c1a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b410e186-9bd8-40bc-a28a-cbd9bc2a4bac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.13.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ed506077-9ccf-49fa-81d1-4b57d5a62bf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in /opt/anaconda3/lib/python3.12/site-packages (4.11.0.86)\n",
      "Requirement already satisfied: numpy>=1.21.2 in /opt/anaconda3/lib/python3.12/site-packages (from opencv-python) (1.26.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python\n",
    "\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "28ed708b-2294-4f3c-9116-4adaed0348db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9103c0d1-ad71-43ac-b344-6d40f1d1c17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import mediapipe as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "852c174c-4ca4-43cb-857b-00c6542465f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6df5eda0-00d5-4478-8177-bc4ac202e09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_holistics=mp.solutions.holistic\n",
    "mp_drawing=mp.solutions.drawing_utils\n",
    "#mp_face_mesh = mp.solutions.face_mesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "75b45060-3046-481c-ad7b-698c7b66e994",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mediapipe_detect(image, model):\n",
    "    # Convert the image from BGR (OpenCV format) to RGB (required by Mediapipe)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Disable writing to the image to improve performance\n",
    "    image.flags.writeable = False\n",
    "    \n",
    "    # Process the image using the Mediapipe model\n",
    "    result = model.process(image)\n",
    "    \n",
    "    # Enable writing to the image again\n",
    "    image.flags.writeable = True\n",
    "    \n",
    "    # Convert the image back to BGR format for OpenCV\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    \n",
    "    return image, result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "383c7d32-74db-4328-9f68-747bc3f05f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_landmark(image, result):\n",
    "        mp_drawing.draw_landmarks(image, result.face_landmarks, mp_holistics.FACEMESH_TESSELATION,\n",
    "                                 mp_drawing.DrawingSpec(color=(80,110,10),thickness=1,circle_radius=1),\n",
    "                                 mp_drawing.DrawingSpec(color=(80,256,121),thickness=1,circle_radius=1)\n",
    "                                 )\n",
    "    \n",
    "        mp_drawing.draw_landmarks(image, result.pose_landmarks, mp_holistics.POSE_CONNECTIONS,\n",
    "                                 mp_drawing.DrawingSpec(color=(80,110,10),thickness=2,circle_radius=3),\n",
    "                                 mp_drawing.DrawingSpec(color=(80,256,121),thickness=2,circle_radius=3)\n",
    "                                 )\n",
    "    \n",
    "        mp_drawing.draw_landmarks(image, result.left_hand_landmarks, mp_holistics.HAND_CONNECTIONS,\n",
    "                                 mp_drawing.DrawingSpec(color=(80,110,10),thickness=2,circle_radius=3),\n",
    "                                 mp_drawing.DrawingSpec(color=(80,256,121),thickness=2,circle_radius=3)\n",
    "                                 )\n",
    "    \n",
    "        mp_drawing.draw_landmarks(image, result.right_hand_landmarks, mp_holistics.HAND_CONNECTIONS,\n",
    "                                 mp_drawing.DrawingSpec(color=(80,110,10),thickness=2,circle_radius=3),\n",
    "                                 mp_drawing.DrawingSpec(color=(80,256,121),thickness=2,circle_radius=3)\n",
    "                                 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3dd91dc8-04a2-467c-bdba-d23a1f5fc3aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1742315312.070288       1 gl_context.cc:344] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M2\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "with mp_holistics.Holistic(min_detection_confidence=0.3, min_tracking_confidence=0.5,model_complexity=1) as holistic:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        if not ret:\n",
    "            print(\"Failed to grab frame\")\n",
    "            break\n",
    "        \n",
    "        # Run Mediapipe detection\n",
    "        frame, result = mediapipe_detect(frame, holistic)\n",
    "\n",
    "        # Draw landmarks on the original frame\n",
    "        draw_landmark(frame, result)\n",
    "\n",
    "        # Show the frame with landmarks drawn\n",
    "        cv2.imshow('OpenCV Feed', frame)\n",
    "\n",
    "        # Exit on pressing 'q'\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "# Release the resources\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4d4f7081-6cf4-404c-b504-c76369d2d9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "pose = np.array([[res.x, res.y, res.z, res.visibility] for res in result.pose_landmarks.landmark]).flatten() if result.pose_landmarks else np.zeros(132)\n",
    "face = np.array([[res.x, res.y, res.z] for res in result.face_landmarks.landmark]).flatten() if result.face_landmarks else np.zeros(1404)\n",
    "lh = np.array([[res.x, res.y, res.z] for res in result.left_hand_landmarks.landmark]).flatten() if result.left_hand_landmarks else np.zeros(21*3)\n",
    "rh = np.array([[res.x, res.y, res.z] for res in result.right_hand_landmarks.landmark]).flatten() if result.right_hand_landmarks else np.zeros(21*3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4724be41-d614-4f10-9ec2-ccbac3ac74be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5.89817047e-01,  3.79830003e-01, -8.17637503e-01,  9.99969721e-01,\n",
       "        6.09285414e-01,  3.12309653e-01, -7.32707858e-01,  9.99950111e-01,\n",
       "        6.24033928e-01,  3.14297736e-01, -7.32707322e-01,  9.99949336e-01,\n",
       "        6.37911439e-01,  3.16460729e-01, -7.32257962e-01,  9.99952137e-01,\n",
       "        5.63389242e-01,  3.11549038e-01, -7.65522361e-01,  9.99950826e-01,\n",
       "        5.46316385e-01,  3.12030822e-01, -7.65302658e-01,  9.99949157e-01,\n",
       "        5.27427971e-01,  3.12913775e-01, -7.65673935e-01,  9.99949694e-01,\n",
       "        6.51144981e-01,  3.48781973e-01, -2.86582261e-01,  9.99945581e-01,\n",
       "        4.98839974e-01,  3.44896376e-01, -4.22616780e-01,  9.99964654e-01,\n",
       "        6.07317328e-01,  4.54247087e-01, -6.53935730e-01,  9.99967575e-01,\n",
       "        5.58934987e-01,  4.52032059e-01, -6.93790615e-01,  9.99971569e-01,\n",
       "        7.40083694e-01,  7.42517352e-01,  7.58342594e-02,  9.99609590e-01,\n",
       "        3.86550248e-01,  7.25431740e-01, -3.42800051e-01,  9.98031259e-01,\n",
       "        9.07399297e-01,  1.04520810e+00, -3.67504627e-01,  8.35608542e-01,\n",
       "        3.32461715e-01,  1.26861405e+00, -7.05997825e-01,  4.42402542e-01,\n",
       "        9.11204219e-01,  5.82823753e-01, -1.17927229e+00,  8.38541627e-01,\n",
       "        5.39655507e-01,  1.12607205e+00, -1.31737459e+00,  4.72869009e-01,\n",
       "        9.18189347e-01,  4.19206887e-01, -1.30542231e+00,  8.37181270e-01,\n",
       "        5.96682847e-01,  1.13122046e+00, -1.45335174e+00,  5.16840816e-01,\n",
       "        8.92775238e-01,  3.99955034e-01, -1.22320139e+00,  8.49164009e-01,\n",
       "        6.01984560e-01,  1.04138720e+00, -1.44375396e+00,  6.03428900e-01,\n",
       "        8.77121150e-01,  4.49356824e-01, -1.16192102e+00,  8.45943809e-01,\n",
       "        5.94845712e-01,  1.03022039e+00, -1.32742763e+00,  5.94348252e-01,\n",
       "        6.84062064e-01,  1.62731421e+00,  5.72825670e-02,  4.31698235e-03,\n",
       "        4.35016453e-01,  1.61970973e+00, -5.26765212e-02,  2.52952357e-03,\n",
       "        6.60986185e-01,  2.39293623e+00,  1.94544978e-02,  6.00148051e-04,\n",
       "        4.22942579e-01,  2.36849380e+00, -1.61751345e-01,  1.97433445e-04,\n",
       "        6.33883119e-01,  3.06226373e+00,  4.98070419e-01,  4.54820984e-05,\n",
       "        4.21174884e-01,  3.04850578e+00,  1.74299613e-01,  4.83938447e-06,\n",
       "        6.31969452e-01,  3.16024351e+00,  5.11538863e-01,  4.69390725e-05,\n",
       "        4.05337840e-01,  3.14876366e+00,  1.85946956e-01,  1.42764493e-05,\n",
       "        6.26345515e-01,  3.24466443e+00, -8.85263644e-03,  1.23691556e-04,\n",
       "        4.86836880e-01,  3.23946142e+00, -4.13237780e-01,  6.77681019e-05])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "488a393a-c1d9-4459-97dd-71cc5866c0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "lh = (np.array([[res.x, res.y, res.z, res.visibility] for res in result.left_hand_landmarks.landmark]).flatten()\n",
    "     if result.right_hand_landmarks else np.zeros(21*3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "91bd9d54-8af2-4b0e-8306-e93ab1fea4da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dca1aa52-4881-4cc9-8fc5-315b015b8a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "rh=(np.array([[res.x,res.y,res.z,res.visibility] for res in result.right_hand_landmarks]).flatten()\n",
    "   if result.right_hand_landmarks else np.zeros(21*3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f07372f7-ad56-460b-adcf-946e91ee7b29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4d11192e-f1f9-4868-99c9-4b9ba1d5bbce",
   "metadata": {},
   "outputs": [],
   "source": [
    "face=(np.array([[res.x,res.y,res.z,res.visibility] for res in result.face_landmarks.landmark]).flatten()\n",
    "     if result.face_landmarks else np.zeros(468*3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "74f16f3f-f8c4-4575-80e5-817a15417209",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.59434378,  0.40655106, -0.02812148, ...,  0.29426557,\n",
       "        0.02703067,  0.        ])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ec2bb6d1-7a81-4507-8976-95af31395279",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keypoints(results):\n",
    "    # Define normalization helper function\n",
    "    def normalize_landmarks(landmarks, scale=1.0):\n",
    "        if len(landmarks) == 0:\n",
    "            return landmarks  # If empty, return as is\n",
    "        # Convert to numpy array and reshape if needed\n",
    "        landmarks = np.array(landmarks)\n",
    "        # Calculate the mean and standard deviation\n",
    "        mean = np.mean(landmarks, axis=0)\n",
    "        std = np.std(landmarks, axis=0)\n",
    "        # Normalize (subtract mean and divide by std)\n",
    "        normalized_landmarks = (landmarks - mean) / (std + 1e-6)  # Add small epsilon to avoid division by zero\n",
    "        return normalized_landmarks * scale\n",
    "\n",
    "    # Extract and normalize keypoints\n",
    "    pose = normalize_landmarks([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]) if results.pose_landmarks else np.zeros((33, 4))\n",
    "    face = normalize_landmarks([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]) if results.face_landmarks else np.zeros((468, 3))\n",
    "    lh = normalize_landmarks([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]) if results.left_hand_landmarks else np.zeros((21, 3))\n",
    "    rh = normalize_landmarks([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]) if results.right_hand_landmarks else np.zeros((21, 3))\n",
    "\n",
    "    # Flatten and concatenate the keypoints\n",
    "    pose = pose.flatten()\n",
    "    face = face.flatten()\n",
    "    lh = lh.flatten()\n",
    "    rh = rh.flatten()\n",
    "\n",
    "    return np.concatenate([pose, face, lh, rh])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "481b8d79-99d2-49dc-82f7-521387796820",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path for exported data, numpy arrays\n",
    "DATA_PATH = os.path.expanduser('~/Documents/MP_Data') \n",
    "\n",
    "# Actions that we try to detect\n",
    "actions = np.array(['hello', 'thanks', 'nice','family','house','I','am','sorry'])\n",
    "\n",
    "# Thirty videos worth of data\n",
    "no_sequences = 30\n",
    "\n",
    "# Videos are going to be 30 frames in length\n",
    "sequence_length = 30\n",
    "\n",
    "# Folder start\n",
    "start_folder = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "edea2596-952b-4d4a-8182-89c76355b582",
   "metadata": {},
   "outputs": [],
   "source": [
    "for action in actions: \n",
    "    for sequence in range(no_sequences):\n",
    "        try: \n",
    "            os.makedirs(os.path.join(DATA_PATH, action, str(sequence)))\n",
    "        except:\n",
    "            pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f454f9fc-3bcf-420b-8d43-3178b565e473",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1742315433.279409       1 gl_context.cc:344] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M2\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "# Set mediapipe model \n",
    "with mp_holistics.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    \n",
    "    # NEW LOOP\n",
    "    # Loop through actions\n",
    "    for action in actions:\n",
    "        # Loop through sequences aka videos\n",
    "        for sequence in range(no_sequences):\n",
    "            # Loop through video length aka sequence length\n",
    "            for frame_num in range(sequence_length):\n",
    "\n",
    "                # Read feed\n",
    "                ret, frame = cap.read()\n",
    "\n",
    "                # Make detections\n",
    "                image, results = mediapipe_detect(frame, holistic)\n",
    "#                 print(results)\n",
    "\n",
    "                # Draw landmarks\n",
    "                draw_landmark(image, results)\n",
    "                \n",
    "                # NEW Apply wait logic\n",
    "                if frame_num == 0: \n",
    "                    cv2.putText(image, 'STARTING COLLECTION', (120,200), \n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255, 0), 4, cv2.LINE_AA)\n",
    "                    cv2.putText(image, 'Collecting frames for {} Video Number {}'.format(action, sequence), (25,22), \n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "                    # Show to screen\n",
    "                    cv2.imshow('OpenCV Feed', image)\n",
    "                    cv2.waitKey(2000)\n",
    "                else: \n",
    "                    cv2.putText(image, 'Collecting frames for {} Video Number {}'.format(action, sequence), (25,22), \n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "                    # Show to screen\n",
    "                    cv2.imshow('OpenCV Feed', image)\n",
    "                \n",
    "                # NEW Export keypoints\n",
    "                keypoints = extract_keypoints(results)\n",
    "                npy_path = os.path.join(DATA_PATH, action, str(sequence), str(frame_num))\n",
    "                np.save(npy_path, keypoints)\n",
    "\n",
    "                # Break gracefully\n",
    "                if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "                    break\n",
    "                    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "65dbc063-77be-47df-841e-419c71456dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "34ed1c67-fad0-459c-9cce-df78f0ed74a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {label:num for num, label in enumerate (actions)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fab9f167-58f3-4c9f-a5bd-70d071f84daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences,labels=[],[] #\n",
    "for action in actions:\n",
    "    for sequence in range(no_sequences):\n",
    "        window=[]\n",
    "        for frame_no in range(sequence_length):\n",
    "            res=np.load(os.path.join(DATA_PATH,action,str(sequence),\"{}.npy\".format(frame_no)))\n",
    "            window.append(res)\n",
    "        sequences.append(window)\n",
    "        labels.append(label_map[action]);  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e00a1989-9bbd-4ee8-8aca-d2699b536612",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=np.array(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6c99fd46-7761-409b-86da-d8b207fdb1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "y=to_categorical(labels).astype(int) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d957a2ed-15fa-4447-922e-5d9786381369",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(240, 30, 1662)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(sequences).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5f3fde6b-a413-434b-8dc6-9b5568584e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "47a04ebf-d7b6-4966-92fb-2237064d689c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(240, 30, 1662)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "85fce1ff-0a8b-4240-801a-440b7bd37c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = to_categorical(labels).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "48390301-542d-4809-b08d-4c3a9673e73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2aefec72-2232-4049-8890-f7f8949bdee7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, 8)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5acdecb1-94f7-40a2-8a96-ffdfef349fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ed6bed4e-c900-41da-bb86-7dcd5127c484",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = os.path.join('Logs')\n",
    "tb_callback = TensorBoard(log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "684c6a01-ec06-43e5-bbe9-f9f5db855146",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.optimizers.legacy import Adam\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(64, return_sequences=True, activation='tanh', input_shape=(30, 1662)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(128, return_sequences=True, activation='tanh'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(64, return_sequences=False, activation='tanh'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(actions.shape[0], activation='softmax'))\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "7821f31b-e6a3-4f99-804f-b7463c4be5c2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "8/8 [==============================] - 3s 109ms/step - loss: 2.1252 - categorical_accuracy: 0.1272 - val_loss: 2.0598 - val_categorical_accuracy: 0.1667 - lr: 0.0010\n",
      "Epoch 2/200\n",
      "8/8 [==============================] - 0s 25ms/step - loss: 2.1233 - categorical_accuracy: 0.1184 - val_loss: 2.0742 - val_categorical_accuracy: 0.1667 - lr: 0.0010\n",
      "Epoch 3/200\n",
      "8/8 [==============================] - 0s 25ms/step - loss: 2.0504 - categorical_accuracy: 0.1842 - val_loss: 1.9716 - val_categorical_accuracy: 0.2500 - lr: 0.0010\n",
      "Epoch 4/200\n",
      "8/8 [==============================] - 0s 25ms/step - loss: 2.0596 - categorical_accuracy: 0.1360 - val_loss: 1.9461 - val_categorical_accuracy: 0.3333 - lr: 0.0010\n",
      "Epoch 5/200\n",
      "8/8 [==============================] - 0s 25ms/step - loss: 1.9917 - categorical_accuracy: 0.2193 - val_loss: 2.0272 - val_categorical_accuracy: 0.1667 - lr: 0.0010\n",
      "Epoch 6/200\n",
      "8/8 [==============================] - 0s 25ms/step - loss: 1.9116 - categorical_accuracy: 0.2456 - val_loss: 1.9061 - val_categorical_accuracy: 0.4167 - lr: 0.0010\n",
      "Epoch 7/200\n",
      "8/8 [==============================] - 0s 25ms/step - loss: 1.7228 - categorical_accuracy: 0.3377 - val_loss: 1.8857 - val_categorical_accuracy: 0.2500 - lr: 0.0010\n",
      "Epoch 8/200\n",
      "8/8 [==============================] - 0s 24ms/step - loss: 1.5284 - categorical_accuracy: 0.3904 - val_loss: 1.6190 - val_categorical_accuracy: 0.3333 - lr: 0.0010\n",
      "Epoch 9/200\n",
      "8/8 [==============================] - 0s 23ms/step - loss: 1.3518 - categorical_accuracy: 0.4386 - val_loss: 1.3926 - val_categorical_accuracy: 0.6667 - lr: 0.0010\n",
      "Epoch 10/200\n",
      "8/8 [==============================] - 0s 25ms/step - loss: 1.1880 - categorical_accuracy: 0.5044 - val_loss: 1.2780 - val_categorical_accuracy: 0.6667 - lr: 0.0010\n",
      "Epoch 11/200\n",
      "8/8 [==============================] - 0s 25ms/step - loss: 1.1438 - categorical_accuracy: 0.5000 - val_loss: 1.2031 - val_categorical_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 12/200\n",
      "8/8 [==============================] - 0s 25ms/step - loss: 1.1300 - categorical_accuracy: 0.4693 - val_loss: 1.4757 - val_categorical_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 13/200\n",
      "8/8 [==============================] - 0s 24ms/step - loss: 0.9793 - categorical_accuracy: 0.6053 - val_loss: 0.9938 - val_categorical_accuracy: 0.6667 - lr: 0.0010\n",
      "Epoch 14/200\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 0.8728 - categorical_accuracy: 0.6184 - val_loss: 0.9528 - val_categorical_accuracy: 0.6667 - lr: 0.0010\n",
      "Epoch 15/200\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 0.8747 - categorical_accuracy: 0.6053 - val_loss: 1.3502 - val_categorical_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 16/200\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 0.8889 - categorical_accuracy: 0.6184 - val_loss: 0.7804 - val_categorical_accuracy: 0.6667 - lr: 0.0010\n",
      "Epoch 17/200\n",
      "8/8 [==============================] - 0s 25ms/step - loss: 1.1965 - categorical_accuracy: 0.5175 - val_loss: 1.4696 - val_categorical_accuracy: 0.4167 - lr: 0.0010\n",
      "Epoch 18/200\n",
      "8/8 [==============================] - 0s 22ms/step - loss: 1.2295 - categorical_accuracy: 0.5000 - val_loss: 1.4361 - val_categorical_accuracy: 0.1667 - lr: 0.0010\n",
      "Epoch 19/200\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 0.9430 - categorical_accuracy: 0.5746 - val_loss: 0.9699 - val_categorical_accuracy: 0.6667 - lr: 0.0010\n",
      "Epoch 20/200\n",
      "8/8 [==============================] - 0s 22ms/step - loss: 0.8008 - categorical_accuracy: 0.6754 - val_loss: 0.9158 - val_categorical_accuracy: 0.6667 - lr: 0.0010\n",
      "Epoch 21/200\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 0.7746 - categorical_accuracy: 0.6974 - val_loss: 0.7643 - val_categorical_accuracy: 0.6667 - lr: 0.0010\n",
      "Epoch 22/200\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 0.6913 - categorical_accuracy: 0.7149 - val_loss: 0.6898 - val_categorical_accuracy: 0.6667 - lr: 0.0010\n",
      "Epoch 23/200\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 0.6402 - categorical_accuracy: 0.7281 - val_loss: 0.7194 - val_categorical_accuracy: 0.8333 - lr: 0.0010\n",
      "Epoch 24/200\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 0.5674 - categorical_accuracy: 0.7719 - val_loss: 0.7331 - val_categorical_accuracy: 0.6667 - lr: 0.0010\n",
      "Epoch 25/200\n",
      "8/8 [==============================] - 0s 23ms/step - loss: 0.5778 - categorical_accuracy: 0.7500 - val_loss: 0.6532 - val_categorical_accuracy: 0.7500 - lr: 0.0010\n",
      "Epoch 26/200\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 0.5220 - categorical_accuracy: 0.7939 - val_loss: 0.4695 - val_categorical_accuracy: 0.7500 - lr: 0.0010\n",
      "Epoch 27/200\n",
      "8/8 [==============================] - 0s 23ms/step - loss: 0.4164 - categorical_accuracy: 0.8640 - val_loss: 0.3654 - val_categorical_accuracy: 0.8333 - lr: 0.0010\n",
      "Epoch 28/200\n",
      "8/8 [==============================] - 0s 25ms/step - loss: 0.4789 - categorical_accuracy: 0.8070 - val_loss: 0.6006 - val_categorical_accuracy: 0.7500 - lr: 0.0010\n",
      "Epoch 29/200\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 0.3782 - categorical_accuracy: 0.8465 - val_loss: 0.2674 - val_categorical_accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 30/200\n",
      "8/8 [==============================] - 0s 25ms/step - loss: 0.2783 - categorical_accuracy: 0.9254 - val_loss: 0.3123 - val_categorical_accuracy: 0.8333 - lr: 0.0010\n",
      "Epoch 31/200\n",
      "8/8 [==============================] - 0s 25ms/step - loss: 0.3521 - categorical_accuracy: 0.8596 - val_loss: 0.5450 - val_categorical_accuracy: 0.8333 - lr: 0.0010\n",
      "Epoch 32/200\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 0.4417 - categorical_accuracy: 0.8333 - val_loss: 0.1381 - val_categorical_accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 33/200\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 0.4394 - categorical_accuracy: 0.8596 - val_loss: 0.2214 - val_categorical_accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 34/200\n",
      "8/8 [==============================] - 0s 25ms/step - loss: 0.3719 - categorical_accuracy: 0.8377 - val_loss: 0.2536 - val_categorical_accuracy: 0.9167 - lr: 0.0010\n",
      "Epoch 35/200\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 0.2530 - categorical_accuracy: 0.9298 - val_loss: 0.1108 - val_categorical_accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 36/200\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 0.1891 - categorical_accuracy: 0.9693 - val_loss: 0.0716 - val_categorical_accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 37/200\n",
      "8/8 [==============================] - 0s 23ms/step - loss: 0.1543 - categorical_accuracy: 0.9649 - val_loss: 0.1008 - val_categorical_accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 38/200\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 0.1959 - categorical_accuracy: 0.9474 - val_loss: 0.0336 - val_categorical_accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 39/200\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 0.1458 - categorical_accuracy: 0.9430 - val_loss: 0.1925 - val_categorical_accuracy: 0.9167 - lr: 0.0010\n",
      "Epoch 40/200\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 0.1149 - categorical_accuracy: 0.9605 - val_loss: 0.1001 - val_categorical_accuracy: 0.9167 - lr: 0.0010\n",
      "Epoch 41/200\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 0.0810 - categorical_accuracy: 0.9868 - val_loss: 0.0395 - val_categorical_accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 42/200\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 0.0897 - categorical_accuracy: 0.9737 - val_loss: 0.0181 - val_categorical_accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 43/200\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 0.0816 - categorical_accuracy: 0.9781 - val_loss: 0.0158 - val_categorical_accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 44/200\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 0.0422 - categorical_accuracy: 1.0000 - val_loss: 0.0205 - val_categorical_accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 45/200\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 0.0462 - categorical_accuracy: 0.9868 - val_loss: 0.0126 - val_categorical_accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 46/200\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 0.0408 - categorical_accuracy: 0.9868 - val_loss: 0.0083 - val_categorical_accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 47/200\n",
      "8/8 [==============================] - 0s 25ms/step - loss: 0.1001 - categorical_accuracy: 0.9605 - val_loss: 0.0086 - val_categorical_accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 48/200\n",
      "8/8 [==============================] - 0s 25ms/step - loss: 0.0824 - categorical_accuracy: 0.9693 - val_loss: 0.2737 - val_categorical_accuracy: 0.9167 - lr: 0.0010\n",
      "Epoch 49/200\n",
      "8/8 [==============================] - 0s 25ms/step - loss: 1.0461 - categorical_accuracy: 0.7281 - val_loss: 0.1497 - val_categorical_accuracy: 0.9167 - lr: 0.0010\n",
      "Epoch 50/200\n",
      "8/8 [==============================] - 0s 25ms/step - loss: 0.8638 - categorical_accuracy: 0.7456 - val_loss: 0.8308 - val_categorical_accuracy: 0.7500 - lr: 0.0010\n",
      "Epoch 51/200\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.6014 - categorical_accuracy: 0.7143\n",
      "Epoch 51: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "8/8 [==============================] - 0s 25ms/step - loss: 0.5942 - categorical_accuracy: 0.7193 - val_loss: 1.0407 - val_categorical_accuracy: 0.9167 - lr: 0.0010\n",
      "Epoch 52/200\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 0.4203 - categorical_accuracy: 0.8114 - val_loss: 0.4065 - val_categorical_accuracy: 0.8333 - lr: 5.0000e-04\n",
      "Epoch 53/200\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 0.3401 - categorical_accuracy: 0.8816 - val_loss: 0.2708 - val_categorical_accuracy: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 54/200\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 0.2369 - categorical_accuracy: 0.9430 - val_loss: 0.2237 - val_categorical_accuracy: 0.9167 - lr: 5.0000e-04\n",
      "Epoch 55/200\n",
      "8/8 [==============================] - 0s 25ms/step - loss: 0.2056 - categorical_accuracy: 0.9430 - val_loss: 0.1487 - val_categorical_accuracy: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 56/200\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.1517 - categorical_accuracy: 0.9821\n",
      "Epoch 56: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "8/8 [==============================] - 0s 24ms/step - loss: 0.1527 - categorical_accuracy: 0.9825 - val_loss: 0.0858 - val_categorical_accuracy: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 57/200\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 0.1060 - categorical_accuracy: 0.9868 - val_loss: 0.0697 - val_categorical_accuracy: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 58/200\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 0.0893 - categorical_accuracy: 0.9912 - val_loss: 0.0567 - val_categorical_accuracy: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 59/200\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 0.0733 - categorical_accuracy: 0.9956 - val_loss: 0.0476 - val_categorical_accuracy: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 60/200\n",
      "8/8 [==============================] - 0s 25ms/step - loss: 0.0725 - categorical_accuracy: 0.9956 - val_loss: 0.0417 - val_categorical_accuracy: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 61/200\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.0644 - categorical_accuracy: 0.9911\n",
      "Epoch 61: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 0.0643 - categorical_accuracy: 0.9912 - val_loss: 0.0378 - val_categorical_accuracy: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 62/200\n",
      "8/8 [==============================] - 0s 24ms/step - loss: 0.0543 - categorical_accuracy: 1.0000 - val_loss: 0.0353 - val_categorical_accuracy: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 63/200\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 0.0529 - categorical_accuracy: 1.0000 - val_loss: 0.0328 - val_categorical_accuracy: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 64/200\n",
      "8/8 [==============================] - 0s 24ms/step - loss: 0.0582 - categorical_accuracy: 1.0000 - val_loss: 0.0318 - val_categorical_accuracy: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 65/200\n",
      "8/8 [==============================] - 0s 24ms/step - loss: 0.0471 - categorical_accuracy: 0.9956 - val_loss: 0.0311 - val_categorical_accuracy: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 66/200\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.0477 - categorical_accuracy: 1.0000Restoring model weights from the end of the best epoch: 46.\n",
      "\n",
      "Epoch 66: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "8/8 [==============================] - 0s 24ms/step - loss: 0.0473 - categorical_accuracy: 1.0000 - val_loss: 0.0297 - val_categorical_accuracy: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 66: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x3feb9fe80>"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping , ReduceLROnPlateau\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True, verbose=1)\n",
    "lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, verbose=1)\n",
    "\n",
    "model.fit(X_train, y_train, epochs=200, batch_size=32, validation_data=(X_test, y_test), callbacks=[early_stopping,lr_scheduler])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "b1bb83a6-fe9c-432e-9fd7-454529a66f49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_27 (LSTM)              (None, 30, 64)            442112    \n",
      "                                                                 \n",
      " dropout_27 (Dropout)        (None, 30, 64)            0         \n",
      "                                                                 \n",
      " lstm_28 (LSTM)              (None, 30, 128)           98816     \n",
      "                                                                 \n",
      " dropout_28 (Dropout)        (None, 30, 128)           0         \n",
      "                                                                 \n",
      " lstm_29 (LSTM)              (None, 64)                49408     \n",
      "                                                                 \n",
      " dropout_29 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_30 (Dense)            (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_31 (Dense)            (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_32 (Dense)            (None, 8)                 264       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 596840 (2.28 MB)\n",
      "Trainable params: 596840 (2.28 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "88dbd37c-3b26-498e-9253-696a5667a84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import multilabel_confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "e0a8d3f7-95a9-44b8-8b7e-79b70980a631",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 20ms/step\n"
     ]
    }
   ],
   "source": [
    "yhat = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "116506bb-dfe4-4cbd-ad2b-ba147af4c573",
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrue = np.argmax(y_test, axis=1).tolist()\n",
    "yhat = np.argmax(yhat, axis=1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7e0775-e483-4747-9866-8f1f306657cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "5b3a380d-b637-478c-9288-14954957c8ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(ytrue, yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86dfa53-9be0-4372-b945-e0ef4f939b1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57dbe54-5ba5-4fba-99ba-f4cae8eba17c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5884ec-a55d-4d18-92a3-1e7c98e45359",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "7d7f7ac5-978b-4e4a-a20d-9e6ab4a1c108",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/my_tensorflow_env/lib/python3.8/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "model.save('Gesture_Recognition.h5');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "9152fa1a-0306-4a67-9342-7de178901831",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/adityavibhute\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "2bf8d79a-1873-4552-888e-4d9fe263e7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\t\n",
    "def prob_viz(res, actions, input_frame, colors):\n",
    "    output_frame = input_frame.copy()\n",
    "    num_classes = min(len(res), len(colors))  # Ensure we don’t exceed the colors list\n",
    "\n",
    "    for num in range(num_classes):  \n",
    "        prob = res[num]  # Get probability\n",
    "        cv2.rectangle(output_frame, (0, 60 + num * 40), (int(prob * 100), 90 + num * 40), colors[num], -1)\n",
    "        cv2.putText(output_frame, actions[num], (0, 85 + num * 40), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "    return output_frame\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "95c68d2d-5589-43f2-95c3-8f167e712b17",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1742352616.434174       1 gl_context.cc:344] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 424ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "hello\n"
     ]
    }
   ],
   "source": [
    "sequence = []\n",
    "sentence = []\n",
    "threshold = 0.95\n",
    "from tensorflow.keras.models import load_model\n",
    "model=load_model('Gesture_Recognition.h5')\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Set mediapipe model \n",
    "with mp_holistics.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened():\n",
    "\n",
    "        # Read feed\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        # Make detections\n",
    "        image, results = mediapipe_detect(frame, holistic)\n",
    "        print(results)\n",
    "        \n",
    "        # Draw landmarks\n",
    "        draw_landmark(image, results)\n",
    "        \n",
    "        # 2. Prediction logic\n",
    "        keypoints = extract_keypoints(results)\n",
    "#         sequence.insert(0,keypoints)\n",
    "#         sequence = sequence[:30]\n",
    "        sequence.append(keypoints)\n",
    "        sequence = sequence[-30:]\n",
    "        \n",
    "        if len(sequence) == 30:\n",
    "            res = model.predict(np.expand_dims(sequence, axis=0))[0]\n",
    "            print(actions[np.argmax(res)])\n",
    "            \n",
    "            \n",
    "        #3. Viz logic\n",
    "            if res[np.argmax(res)] > threshold: \n",
    "                if len(sentence) > 0: \n",
    "                    if actions[np.argmax(res)] != sentence[-1]:\n",
    "                        sentence.append(actions[np.argmax(res)])\n",
    "                else:\n",
    "                    sentence.append(actions[np.argmax(res)])\n",
    "\n",
    "            if len(sentence) > 5: \n",
    "                sentence = sentence[-5:]\n",
    "\n",
    "            # Viz probabilities\n",
    "            image = prob_viz(res, actions, image, colors)\n",
    "            \n",
    "        cv2.rectangle(image, (0,0), (640, 40), (245, 117, 16), -1)\n",
    "        cv2.putText(image, ' '.join(sentence), (3,30), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        \n",
    "        # Show to screen\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "\n",
    "        # Break gracefully\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "37566ab6-2e4a-42bf-9fd9-ce5359329107",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01cbb95c-3228-44b2-9518-c62882ed10ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#American Hand Sign Language Detection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "594c9a16-d3e4-44c3-9fe5-832ce308ab2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fb91485f-8075-4095-88ab-12954d194043",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Save the features (X) and labels (y_categorical) as .npy files\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m np\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mX_features.npy\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[43mX\u001b[49m)\n\u001b[1;32m      5\u001b[0m np\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124my_labels.npy\u001b[39m\u001b[38;5;124m'\u001b[39m, y_categorical)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData saved successfully!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Save the features (X) and labels (y_categorical) as .npy files\n",
    "np.save('X_features.npy', X)\n",
    "np.save('y_labels.npy', y_categorical)\n",
    "\n",
    "print(\"Data saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "30a6e5a1-85fb-4003-9205-4b03989e7283",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 30, 64)            442112    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 30, 64)            0         \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 30, 128)           98816     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 30, 128)           0         \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 64)                49408     \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 3)                 99        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 596675 (2.28 MB)\n",
      "Trainable params: 596675 (2.28 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "model=load_model('first_try.h5')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c60ebb-ec26-4fed-8e79-8f3aed46df9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "model=load_model('first_try.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "6f914ebc-637e-4c21-884b-24c615839ce0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution: [30 28 28 29 27 30 29 27]\n"
     ]
    }
   ],
   "source": [
    "class_distribution = np.sum(y_train, axis=0)\n",
    "print(\"Class distribution:\", class_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "8d89b373-d4e2-481c-8a2a-469cf2d7a3f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12, 8)\n"
     ]
    }
   ],
   "source": [
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "9509b179-344f-4649-887b-7a24e232ef55",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[111], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43myhat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "print(yhat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f80ed95-8430-455b-8684-269f5b2eaacf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
